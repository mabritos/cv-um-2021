{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Face Detection Test Evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolasrondan/cv-um-2021/blob/main/trabajo-final/Face_Detection_Test_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nrqo2QCAeQi"
      },
      "source": [
        "#@title \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Mxh8_O8Cv3d"
      },
      "source": [
        "%cd /content/drive/MyDrive/computer-vision-um/cv-um-2021/trabajo-final/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b6RY0B1Z-zR"
      },
      "source": [
        "import cv2 \n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from enum import Enum\n",
        "import os\n",
        "import sklearn \n",
        "import sklearn.neighbors\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from evaluation import evaluate_detector, precision_and_recall, interpolated_average_precision\n",
        "import sys\n",
        "from image_utils import non_max_suppression\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "import math\n",
        "from skimage import feature\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uil6sPuXAeQl"
      },
      "source": [
        "test_files_dir = './test_files/data/face_detection/te_raw_images/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqT5ESifYPdU"
      },
      "source": [
        "### Feature Extractor Code "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP6kJr6qYUI9"
      },
      "source": [
        "class FeatureExtractors(Enum):\n",
        "\t\tMiniImage = 1\n",
        "\t\tHOG = 2\n",
        "\t\tLBP = 3\n",
        "\n",
        "def extract_features(method, img):\n",
        "\t'''Switch between Feature extraction Methods'''\n",
        "\n",
        "\timage_representation = []\n",
        "\n",
        "\tif method == FeatureExtractors.MiniImage:\n",
        "\t\timage_representation = extract_mini_image_features(img)\n",
        "\telif method == FeatureExtractors.HOG:\n",
        "\t\timage_representation = extract_hog_features(img)\n",
        "\telif method == FeatureExtractors.LBP:\n",
        "\t\timage_representation = extract_lbp_features(img)\t\n",
        "\t\n",
        "\treturn image_representation\n",
        "\n",
        "def extract_mini_image_features(img,resize_size=(64,64)):\n",
        "\tresized_image = cv2.resize(img,resize_size)\n",
        "\timage_representation = resized_image.reshape(resize_size[0]*resize_size[1])\n",
        "\treturn image_representation\n",
        "  \n",
        "def extract_lbp_features(img):\n",
        "  return []\n",
        "\n",
        "def extract_hog_features(img):\n",
        "  return []\n",
        "\n",
        "\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0AUVHYWaokB"
      },
      "source": [
        "def sliding_window(image, window_size, scale, stride):\n",
        "    [image_rows, image_cols] = image.shape;\n",
        "    window_rows = window_size[0];\n",
        "    window_cols = window_size[1];\n",
        "\n",
        "    patches = np.zeros((window_rows, window_cols,5));\n",
        "    bbox_locations = np.zeros((5,4))\n",
        "    r = np.random.randint(0,image_rows-window_rows,5); # Sample top left position\n",
        "    c = np.random.randint(0,image_cols-window_cols,5);\n",
        "    for i in range(0,5):\n",
        "        patches[:,:,i] = image[r[i]:r[i]+window_rows, c[i]:c[i]+window_cols];\n",
        "        bbox_locations[i,:] = [r[i],c[i],window_rows,window_cols]; # top-left y,x, height, width\n",
        "\n",
        "\n",
        "    return patches, bbox_locations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL8tiUezayNh"
      },
      "source": [
        "def show_image_with_bbox(image,bboxes,draw_GT=True):\n",
        "    GT = [82,91,166,175]\n",
        "    if draw_GT:\n",
        "        cv2.rectangle(image, (GT[0],GT[1]), (GT[2],GT[3]), (0, 0, 255), 2)\n",
        "\n",
        "    for bbox in bboxes:\n",
        "        if len(bbox) == 4:   \n",
        "            top_left = (int(bbox[0]),int(bbox[1]))\n",
        "            bottom_right = (int(bbox[0])+ int(bbox[2]),int(bbox[1])+int(bbox[3]))\n",
        "            cv2.rectangle(image, top_left, bottom_right, (255, 0, 0), 2)\n",
        "\n",
        "    plt.imshow(image[...,::-1])\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usf-r9ImDNcp"
      },
      "source": [
        "### Load Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZpbckFHAeQl"
      },
      "source": [
        "# IF using sklearn\n",
        "classifier = pickle.load(open('./face_detector','rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vhk1gt1lDLDx"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At6Uj3lBAeQm"
      },
      "source": [
        "total_true_positives = []\n",
        "total_real_positives = []\n",
        "total_positive_predictions = []\n",
        "window_size = [64, 64]\n",
        "folders = sorted(glob(test_files_dir + '/*'))\n",
        "for subject_folder in tqdm(folders,total=len(folders)):\n",
        "    for img in sorted(glob(subject_folder + '/*.jpg')):\n",
        "        bgr_image = cv2.imread(img)\n",
        "        gray_image = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2GRAY)\n",
        "        patches, bbox_locations = sliding_window(gray_image,window_size,1,32)\n",
        "        ## You need to extract features for every patch (same features you used for training the classifier)\n",
        "        patches_feature_representation = []\n",
        "        for i in range(patches.shape[2]):\n",
        "            patch_representation = extract_features(FeatureExtractors.HOG, patches[:,:,i])\n",
        "            patches_feature_representation.append(patch_representation)\n",
        "        patches_feature_representation = np.asarray(patches_feature_representation)\n",
        "        ## Get score for each sliding window patch\n",
        "        scores = classifier.predict_proba(patches_feature_representation)\n",
        "        ## Positive Face Probabilities\n",
        "        face_probabilities = scores[:,1]\n",
        "        #[labels, acc, prob] = predict([],patches_feature_representation, clasifier)\n",
        "        ## Positive Face Probabilities\n",
        "        #face_probabilities = np.asarray(prob)\n",
        "        #face_probabilities = face_probabilities.T[0]\n",
        "        \n",
        "        [ detected_true_positives, image_real_positives, detected_faces ] = evaluate_detector( bbox_locations, face_probabilities);\n",
        "        total_true_positives.append(detected_true_positives)\n",
        "        total_real_positives.append(image_real_positives)\n",
        "        total_positive_predictions.append(detected_faces)\n",
        "        \n",
        "total_true_positives = np.asarray(total_true_positives)\n",
        "total_real_positives = np.asarray(total_real_positives)\n",
        "total_positive_predictions = np.asarray(total_positive_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAWftccHAeQn"
      },
      "source": [
        "precision, recall = precision_and_recall(total_true_positives, total_real_positives,total_positive_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Io8CrHkXEMsS"
      },
      "source": [
        "plt.plot(recall, precision)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.xlim(0,1.1)\n",
        "plt.ylim(0,1.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S-kjuO4AeQo"
      },
      "source": [
        "ap = interpolated_average_precision(recall,precision)\n",
        "print('Detection Average Precision is {}'.format(ap))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuI2ExNzAeQp"
      },
      "source": [
        "#### VISUALIZE RESULTS #### "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9z3Is-IAeQp"
      },
      "source": [
        "window_size = [64, 64]\n",
        "predictions = []\n",
        "threshold_p = 0.5\n",
        "overlap_threshold = 0.5\n",
        "folders = sorted(glob(test_files_dir + '/*'))\n",
        "for subject_folder in tqdm(folders,total=len(folders)):\n",
        "    for img_file in sorted(glob(subject_folder + '/*.jpg')):\n",
        "      bgr_image = cv2.imread(img_file)\n",
        "      gray_image = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2GRAY)\n",
        "      patches, bbox_locations = sliding_window(gray_image,window_size,1,32)\n",
        "\n",
        "      ## You need to extract features for every patch (same features you used for training the classifier)\n",
        "      patches_feature_representation = []\n",
        "      for i in range(patches.shape[2]):\n",
        "          patch_representation = extract_features(FeatureExtractors.HOG, patches[:,:,i])\n",
        "          patches_feature_representation.append(patch_representation)\n",
        "      patches_feature_representation = np.asarray(patches_feature_representation)\n",
        "      ## Get prediction label for each sliding window patch\n",
        "      labels = classifier.predict(patches_feature_representation)\n",
        "      ## Get score for each sliding window patch\n",
        "      scores = classifier.predict_proba(patches_feature_representation)\n",
        "      ## Positive Face Probabilities\n",
        "      face_probabilities = scores[:,1]\n",
        "      face_bboxes = bbox_locations[face_probabilities>threshold_p]\n",
        "      face_bboxes_probabilites = face_probabilities[face_probabilities>threshold_p]\n",
        "      # Do non max suppression and select strongest probability box\n",
        "      [selected_bbox, selected_score] = non_max_suppression(face_bboxes,face_bboxes_probabilites,0.3)\n",
        "      show_image_with_bbox(bgr_image, selected_bbox)\n",
        "      print(selected_bbox)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}